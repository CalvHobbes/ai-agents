{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO67BfmAVD8GKlYdEosubdi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalvHobbes/ai-agents/blob/main/Agentic_RAG_with_LLamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jGR-yoMUcRJ"
      },
      "outputs": [],
      "source": [
        "## REF - https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/2/router-query-engine"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file dynamically from GitHub\n",
        "!wget -O agentic_rag_llamaindex_requirements.txt https://raw.githubusercontent.com/CalvHobbes/ai-agents/main/agentic_rag_llamaindex_requirements.txt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRA48HlGUhx_",
        "outputId": "f700b190-9120-46ae-8ed8-dea2c945f1d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-22 11:14:27--  https://raw.githubusercontent.com/CalvHobbes/ai-agents/main/agentic_rag_llamaindex_requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252 [text/plain]\n",
            "Saving to: ‘agentic_rag_llamaindex_requirements.txt’\n",
            "\n",
            "\r          agentic_r   0%[                    ]       0  --.-KB/s               \ragentic_rag_llamain 100%[===================>]     252  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-22 11:14:28 (3.74 MB/s) - ‘agentic_rag_llamaindex_requirements.txt’ saved [252/252]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the requirements\n",
        "!pip install -r agentic_rag_llamaindex_requirements.txt"
      ],
      "metadata": {
        "id": "-Wn-0Ik2uE48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfAb9BqxdlZa",
        "outputId": "b7bc9a4c-dd33-46c2-aec8-f8dc6ff8e648"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-22 11:27:47--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‘metagpt.pdf’\n",
            "\n",
            "metagpt.pdf         100%[===================>]  16.13M  45.9MB/s    in 0.4s    \n",
            "\n",
            "2025-01-22 11:27:47 (45.9 MB/s) - ‘metagpt.pdf’ saved [16911937/16911937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n"
      ],
      "metadata": {
        "id": "ojVb0SKNeITj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "gIUmhpLSfV1i"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAPI_KEY')"
      ],
      "metadata": {
        "id": "NQYtfBmvjR3_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "id": "1zgCJMVBykGt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Settings.llm"
      ],
      "metadata": {
        "id": "0_xxa6ZgRP8s"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "9caNdryxype-"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "MjDkGpkN1xoB"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "rduMKhHE20Z9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "EfQRgNQC3Ix2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwiepNUj3nQu",
        "outputId": "6ea06e33-daba-473b-f209-365e92812f44"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: This choice indicates that the document is useful for summarization questions related to MetaGPT..\n",
            "\u001b[0mThe document introduces MetaGPT, a meta-programming framework that enhances multi-agent systems based on Large Language Models (LLMs) through role specialization, workflow management, and efficient communication mechanisms. It incorporates an executable feedback mechanism to improve code generation quality during runtime and achieves state-of-the-art performance on various benchmarks. The document also provides insights into the development process of a software application called the \"Drawing App\" using MetaGPT, highlighting stages like requirement gathering, UI design, system architecture, implementation approach, testing, and task allocation. It discusses the use of Python libraries, creation of a user-friendly GUI, code generation, unit testing, and performance evaluation. Additionally, it touches upon challenges, ethical concerns, and the impact of MetaGPT on programming accessibility and transparency, while also mentioning potential future directions for the framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"How do agents share information with other agents?\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QSEGAqd4Mfm",
        "outputId": "9f420b99-48bd-43a3-95a4-a1433168c0fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it focuses on retrieving specific context from the MetaGPT paper, which may contain information on how agents share information with other agents..\n",
            "\u001b[0mAgents share information with other agents by utilizing a shared message pool where they can publish structured messages and subscribe to relevant messages based on their profiles. This shared message pool allows all agents to exchange messages directly, enabling them to access messages from other entities transparently. By storing information in this global message pool, agents can retrieve required information without the need to inquire about other agents and wait for their responses, thus enhancing communication efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3yut0C4rlW",
        "outputId": "85156612-23c8-48fe-f1ac-79935046e203"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: Ablation study results are specific context from the MetaGPT paper, making choice 2 the most relevant..\n",
            "\u001b[0mThe ablation study results show that MetaGPT effectively addresses challenges related to context utilization, code hallucinations, and information overload in the development of recommendation engines. By accurately unfolding natural language descriptions, maintaining information validity, and focusing on granular tasks like requirement analysis, MetaGPT mitigates issues such as ambiguity, incomplete code implementation, and irrelevant information. The use of a global message pool and subscription mechanism helps streamline communication and filter out irrelevant contexts, enhancing the efficiency and utility of the information provided.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"summarise ablation study\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMnFNYOw5aNa",
        "outputId": "77f69402-01a0-494f-eb1b-473d1332e479"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: Ablation study typically involves analyzing specific components or features of a model, which aligns more closely with retrieving specific context from the MetaGPT paper..\n",
            "\u001b[0mAn ablation study involves systematically removing or disabling certain components or features of a system to evaluate their individual impact on the overall performance or functionality. This process helps in understanding the importance and contribution of each component towards the system's effectiveness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"who is gabriel cirulli\") # name onlt mentioned in an image in the metagpt pdf\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWRtm0sAJG7j",
        "outputId": "5f02e420-63e4-4578-8632-314313fb614d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it focuses on retrieving specific context, which would be helpful in finding information about Gabriel Cirulli..\n",
            "\u001b[0mGabriel Cirulli is not mentioned in the provided context information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function Calling ##"
      ],
      "metadata": {
        "id": "47LZl0zvCiZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def add(x: int, y: int) -> int:\n",
        "    \"\"\"Adds two integers together.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "def mystery(x: int, y: int) -> int:\n",
        "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
        "    return (x + y) * (x + y)\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(add)\n",
        "mystery_tool = FunctionTool.from_defaults(mystery)"
      ],
      "metadata": {
        "id": "IyPfzpP3Cj2S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm.predict_and_call([add_tool, mystery_tool],\n",
        "    \"mystify 9 and 8\",\n",
        "    verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iGcWoArDFNt",
        "outputId": "afb1bdd8-1867-49cf-f21c-6a250fe29d37"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: mystery with args: {\"x\": 9, \"y\": 8}\n",
            "=== Function Output ===\n",
            "289\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentChatResponse(response='289', sources=[ToolOutput(content='289', tool_name='mystery', raw_input={'args': (), 'kwargs': {'x': 9, 'y': 8}}, raw_output=289, is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UxdrYA7R8Wi",
        "outputId": "60e49336-d6fc-4c49-8dda-86ca047ff758"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nodes[33].get_content(metadata_mode=\"all\"))"
      ],
      "metadata": {
        "id": "Zl6Bko_AIZe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=4)\n"
      ],
      "metadata": {
        "id": "5KgFNhh3JnX-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What are some high-level results of MetaGPT?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4gjSispKK_N",
        "outputId": "2468af33-f8cd-40a2-e9eb-63ebb28dd640"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MetaGPT outperforms all preceding approaches in both HumanEval and MBPP benchmarks. When MetaGPT collaborates with GPT-4, it significantly improves the Pass @k in the HumanEval benchmark compared to GPT-4. It achieves 85.9% and 87.7% pass rates on the MBPP and HumanEval with a single attempt. Additionally, MetaGPT achieves an average score of 3.9, surpassing ChatDev’s score of 2.1, based on the Chat chain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.vector_stores import MetadataFilters\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=2,\n",
        "                                            filters=MetadataFilters.from_dicts(\n",
        "                                                [\n",
        "                                                    {\"key\": \"page_label\", \"value\": \"3\"}\n",
        "\n",
        "                                                  ])\n",
        ")\n"
      ],
      "metadata": {
        "id": "8NjKEvWAKmHN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"What are some high-level results of MetaGPT?\",\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaI7FKirL6zm",
        "outputId": "6d24151d-0f0c-4a97-cf66-f1930ab6fdd8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MetaGPT demonstrates state-of-the-art performance on HumanEval and MBPP, showcasing its effectiveness as a meta-programming framework for developing LLM-based multi-agent systems. Additionally, MetaGPT integrates human-like Standard Operating Procedures (SOPs) to enhance robustness and reduce unproductive collaboration among LLM-based agents. The framework also introduces an executive feedback mechanism that debugs and executes code during runtime, leading to a significant improvement in code generation quality.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHC8JDDYL82Q",
        "outputId": "49f9a30e-42a5-4d92-e632-b2640dac547d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page_label': '3', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}\n",
            "{'page_label': '3', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from llama_index.core.vector_stores import FilterCondition\n",
        "\n",
        "def total_page_count() -> int:\n",
        "  \"\"\"\n",
        "  Returns total number of pages in MetaGPT document, Only used to fetch the numnber of pages of the document\n",
        "  \"\"\"\n",
        "  return 29\n",
        "\n",
        "def vector_query(query:str, page_numbers: List[str]) -> str:\n",
        "  \"\"\"Perform a vector search over an index.\n",
        "  query (str): the string query to be embedded.\n",
        "  page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
        "  over all pages. Otherwise, filter by the set of specified pages.\n",
        "  \"\"\"\n",
        "  filters = [\n",
        "      {\"key\": \"page_label\", \"value\": page} for page in page_numbers\n",
        "  ]\n",
        "  query_engine = vector_index.as_query_engine(similarity_top_k=2,\n",
        "                                            filters=MetadataFilters.from_dicts(\n",
        "                                                filters,\n",
        "                                                condition=FilterCondition.OR\n",
        "                                                  )\n",
        "                                            )\n",
        "  response = query_engine.query(query)\n",
        "  return response\n",
        "\n",
        "vector_query_tool = FunctionTool.from_defaults( name=\"vector_query\",\n",
        "    fn=vector_query)\n",
        "page_count_tool = FunctionTool.from_defaults(name=\"page_count_tool\", fn=total_page_count)"
      ],
      "metadata": {
        "id": "MoL_7nlcMox9"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_and_call([vector_query_tool], \"What are the high-level results of MetaGPT as described on the second page?\", verbose=True)"
      ],
      "metadata": {
        "id": "X0BQr2HTOYiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "# summary_index = SummaryIndex(nodes)\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "Rarow6PkTz8m"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What are the MetaGPT comparisons with ChatDev described on second page?\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCLRGtMWWmtL",
        "outputId": "275b2c90-ecf0-4f20-9ee2-c039d996727b"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"2\"]}\n",
            "=== Function Output ===\n",
            "MetaGPT outperforms ChatDev in handling higher levels of software complexity and offering extensive functionality. In experimental evaluations, MetaGPT achieves a 100% task completion rate, showcasing the robustness and efficiency of its design in terms of time and token costs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zBUlZd2WsAw",
        "outputId": "96843041-c8e7-4e1b-c684-b2d406dad41f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page_label': '8', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"what is the summary of the paper?\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3wOrmOIW3rz",
        "outputId": "0ee3eb20-d71f-45fc-8320-5fbb001a7ceb"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"summary of the paper\"}\n",
            "=== Function Output ===\n",
            "The paper introduces MetaGPT, a meta-programming framework that enhances multi-agent systems based on Large Language Models (LLMs) through Standardized Operating Procedures (SOPs). It incorporates role specialization, workflow management, and efficient communication mechanisms to facilitate collaborative software development. MetaGPT involves agents like Product Managers, Architects, Engineers, and QA Engineers, each with specific roles. The framework utilizes structured communication interfaces, a publish-subscribe mechanism, and an executable feedback mechanism to improve code generation quality. Experimental results demonstrate MetaGPT's effectiveness in software development tasks, outperforming existing approaches in various benchmarks. The paper also discusses the development process of a software application called the \"Drawing App\" using MetaGPT, outlining tasks, system architecture, technical specifications, and implementation approach. It evaluates MetaGPT's performance in generating executable code, addresses challenges, limitations, and ethical concerns, and explores potential future directions for enhancing its capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "id": "JFO-HbcZW7te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "BncKaCfgewAd"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    [vector_tool, summary_tool, page_count_tool],\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ],
      "metadata": {
        "id": "ssTA02-Fb6uO"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the agent roles in MetaGPT, \"\n",
        "    \"and then how they communicate with each other.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "125KdClkdUdt",
        "outputId": "2d9c0a15-0357-4dc2-e173-6fd979308e3d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"Agent roles in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The roles in MetaGPT include the Product Manager responsible for creating the Product Requirement Document and analyzing user stories and competitive analysis, the Architect who designs technical specifications and system architecture diagrams, the Project Manager who breaks down tasks and assigns them to Engineers, the Engineers who develop code based on specifications, and the QA Engineer who generates unit test code and reviews the code for quality assurance. Each agent has a specific role contributing to the success of software projects in MetaGPT.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"Communication between agent roles in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The communication between agent roles in MetaGPT is structured and efficient, with each agent performing specific tasks based on the information provided by the previous agent. The Product Manager generates a Product Requirement Document (PRD) that outlines project goals, user stories, and competitive analysis. This document is then passed to the Architect, who designs the system architecture and interface definitions. The Project Manager breaks down the project into tasks based on the PRD and Architect's specifications, which are then executed by the Engineer. The QA Engineer reviews the code generated by the Engineer and creates unit tests to ensure software quality. This sequential flow of information and tasks ensures a systematic approach to software development within the MetaGPT framework.\n",
            "=== LLM Response ===\n",
            "In MetaGPT, there are several agent roles involved in software development. These roles include the Product Manager, Architect, Project Manager, Engineers, and QA Engineer. Each role has specific responsibilities contributing to the success of software projects within MetaGPT.\n",
            "\n",
            "The Product Manager is responsible for creating the Product Requirement Document (PRD) and analyzing user stories and competitive analysis. The Architect designs technical specifications and system architecture diagrams. The Project Manager breaks down tasks and assigns them to Engineers. The Engineers develop code based on specifications provided by the Architect and Project Manager. The QA Engineer generates unit test code and reviews the code for quality assurance.\n",
            "\n",
            "Communication between these agent roles in MetaGPT is structured and efficient. The Product Manager creates the PRD, which is then passed to the Architect for system architecture design. The Project Manager breaks down tasks based on the PRD and Architect's specifications, which are then executed by the Engineers. The QA Engineer reviews the code generated by the Engineers and creates unit tests to ensure software quality. This sequential flow of information and tasks ensures a systematic approach to software development within the MetaGPT framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.chat(\n",
        "    \"Tell me about the evaluation datasets used.\"\n",
        ")"
      ],
      "metadata": {
        "id": "1AX7Ksyddwx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.chat(\"Tell me the results over one of the above datasets.\")"
      ],
      "metadata": {
        "id": "Iu0c6e-oiFmU",
        "outputId": "8eb050cf-5005-49ee-a20a-003d2857ac90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me the results over one of the above datasets.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"results over HumanEval dataset in MetaGPT document\"}\n",
            "=== Function Output ===\n",
            "GPT-4 consistently outperformed GPT-3.5-Turbo in various settings on the HumanEval dataset, achieving higher scores across different experiments.\n",
            "=== LLM Response ===\n",
            "GPT-4 consistently outperformed GPT-3.5-Turbo in various settings on the HumanEval dataset, achieving higher scores across different experiments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task = agent.create_task(\n",
        "    \"Tell me about the agent roles in MetaGPT, \"\n",
        "    \"and then how they communicate with each other.\"\n",
        ")"
      ],
      "metadata": {
        "id": "oCvCrEOxhU0p"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_output = agent.run_step(task.task_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUkPVjtSd0ja",
        "outputId": "39d4bccd-7c79-4619-82d1-fbe2b8c7e701"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"agent roles in MetaGPT document\"}\n",
            "=== Function Output ===\n",
            "The agent roles in the MetaGPT document include the Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has distinct responsibilities within the software development process. The Product Manager focuses on creating the Product Requirement Document and conducting competitive analysis. The Architect is responsible for designing the system architecture and technical specifications. The Project Manager breaks down the project into tasks for execution. The Engineer develops the code based on provided specifications, while the QA Engineer creates unit tests and ensures software quality. Each agent contributes uniquely to the success of the project by fulfilling their designated role.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"communication between agent roles in MetaGPT document\"}\n",
            "=== Function Output ===\n",
            "The communication between agent roles in MetaGPT is structured and efficient, with each role having a specific set of responsibilities. The Product Manager creates the Product Requirement Document (PRD) which is then passed on to the Architect for system architecture and interface design. The Project Manager further breaks down the project into tasks based on the PRD and Architect's specifications. The Engineer then develops the code according to the assigned tasks, and the QA Engineer ensures software quality through code reviews and unit tests. This structured workflow facilitates clear communication and collaboration among the different agent roles in MetaGPT.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "for key in dict(step_output).keys():\n",
        "    print(key)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kGtrbskhks6n",
        "outputId": "dae936b3-3190-414a-e1ac-ba9a36fcff9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output\n",
            "task_step\n",
            "next_steps\n",
            "is_last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completed_steps = agent.get_completed_steps(task.task_id)\n",
        "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
        "print(completed_steps[0].output.sources[0].raw_output)"
      ],
      "metadata": {
        "id": "moeijd3Hk_HU",
        "outputId": "09966db2-5414-4301-9d3a-860192caf807",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num completed for task 7299ebaf-f69c-4660-a06c-0857a6fe29cb: 1\n",
            "The agent roles in the MetaGPT document include the Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has distinct responsibilities within the software development process. The Product Manager focuses on creating the Product Requirement Document and conducting competitive analysis. The Architect is responsible for designing the system architecture and technical specifications. The Project Manager breaks down the project into tasks for execution. The Engineer develops the code based on provided specifications, while the QA Engineer creates unit tests and ensures software quality. Each agent contributes uniquely to the success of the project by fulfilling their designated role.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
        "print(f\"Num upcoming steps for task {task.task_id}: {len(upcoming_steps)}\")\n",
        "upcoming_steps[0]"
      ],
      "metadata": {
        "id": "tnY2gBhflJOI",
        "outputId": "d7f252e9-2954-4685-b96d-388e43bb412b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num upcoming steps for task 7299ebaf-f69c-4660-a06c-0857a6fe29cb: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskStep(task_id='7299ebaf-f69c-4660-a06c-0857a6fe29cb', step_id='cae33ec5-5744-41e0-827b-c9db51270729', input=None, step_state={}, next_steps={}, prev_steps={}, is_ready=True)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_output = agent.run_step(\n",
        "    task.task_id, input=\"What about how agents share information?\"\n",
        ")"
      ],
      "metadata": {
        "id": "u4G4ZEkbl3sX",
        "outputId": "d6e48fc7-fd3d-4af8-90b6-c9403f17b6a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What about how agents share information?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"how agents share information in MetaGPT document\"}\n",
            "=== Function Output ===\n",
            "Agents in MetaGPT share information through a structured communication protocol that includes a shared message pool and a publish-subscribe mechanism. This allows for direct message exchange among all agents and enables them to subscribe to relevant messages based on their roles. The shared message pool and subscription mechanism enhance communication efficiency by providing a centralized platform for information exchange and ensuring that agents receive only task-related information, thus avoiding information overload.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_output = agent.run_step(task.task_id)\n",
        "print(step_output.is_last)"
      ],
      "metadata": {
        "id": "JxxJIm7pnAiL",
        "outputId": "e0d804ff-1c3d-489d-fc59-f8f55b5feb30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LLM Response ===\n",
            "Agents in MetaGPT share information through a structured communication protocol that includes a shared message pool and a publish-subscribe mechanism. This allows for direct message exchange among all agents and enables them to subscribe to relevant messages based on their roles. The shared message pool and subscription mechanism enhance communication efficiency by providing a centralized platform for information exchange and ensuring that agents receive only task-related information, thus avoiding information overload.\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.finalize_response(task.task_id)\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "j4Sd2IKhotqx",
        "outputId": "e1d6b347-a06c-4ad8-f24b-eafb88291d85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agents in MetaGPT share information through a structured communication protocol that includes a shared message pool and a publish-subscribe mechanism. This allows for direct message exchange among all agents and enables them to subscribe to relevant messages based on their roles. The shared message pool and subscription mechanism enhance communication efficiency by providing a centralized platform for information exchange and ensuring that agents receive only task-related information, thus avoiding information overload.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Document RAG##"
      ],
      "metadata": {
        "id": "eijEoIFsq9en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "]"
      ],
      "metadata": {
        "id": "6Zx4XxV-rAVQ"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "for url, filename in zip(urls, papers):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception for bad responses\n",
        "\n",
        "    # Save the PDF to the 'pdfs' directory\n",
        "    filepath = os.path.join(\"\", filename)\n",
        "    with open(filepath, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(f\"Downloaded {filename} from {url}\")\n",
        "\n",
        "print(\"All PDFs downloaded successfully!\")"
      ],
      "metadata": {
        "id": "IgnJGnJJrOqM",
        "outputId": "cf0d11ed-fb35-49e3-b056-5a69da476fb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded metagpt.pdf from https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Downloaded longlora.pdf from https://openreview.net/pdf?id=6PmJoRfdaK\n",
            "Downloaded selfrag.pdf from https://openreview.net/pdf?id=hSyW5go0v8\n",
            "All PDFs downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional,List\n",
        "from llama_index.core.vector_stores import FilterCondition\n",
        "def get_doc_tools(\n",
        "    file_path: str,\n",
        "    name: str,\n",
        ") -> str:\n",
        "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
        "\n",
        "    # load documents\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "    vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "    def vector_query(\n",
        "        query: str,\n",
        "        page_numbers: Optional[List[str]] = None\n",
        "    ) -> str:\n",
        "        \"\"\"Use to answer questions over a given paper.\n",
        "\n",
        "        Useful if you have specific questions over the paper.\n",
        "        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
        "\n",
        "        Args:\n",
        "            query (str): the string query to be embedded.\n",
        "            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\n",
        "                if we want to perform a vector search\n",
        "                over all pages. Otherwise, filter by the set of specified pages.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        page_numbers = page_numbers or []\n",
        "        metadata_dicts = [\n",
        "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "        ]\n",
        "\n",
        "        query_engine = vector_index.as_query_engine(\n",
        "            similarity_top_k=2,\n",
        "            filters=MetadataFilters.from_dicts(\n",
        "                metadata_dicts,\n",
        "                condition=FilterCondition.OR\n",
        "            )\n",
        "        )\n",
        "        response = query_engine.query(query)\n",
        "        return response\n",
        "\n",
        "\n",
        "    vector_query_tool = FunctionTool.from_defaults(\n",
        "        name=f\"vector_tool_{name}\",\n",
        "        fn=vector_query\n",
        "    )\n",
        "\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        name=f\"summary_tool_{name}\",\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            f\"Useful for summarization questions related to {name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return vector_query_tool, summary_tool"
      ],
      "metadata": {
        "id": "OiIFKVMmroyv"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yLuVKHF1spsC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}