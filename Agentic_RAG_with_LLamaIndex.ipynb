{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqJHVOJK0F7wrI68wJxniY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalvHobbes/ai-agents/blob/main/Agentic_RAG_with_LLamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jGR-yoMUcRJ"
      },
      "outputs": [],
      "source": [
        "## REF - https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/2/router-query-engine"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file dynamically from GitHub\n",
        "!wget -O agentic_rag_llamaindex_requirements.txt https://raw.githubusercontent.com/CalvHobbes/ai-agents/main/agentic_rag_llamaindex_requirements.txt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRA48HlGUhx_",
        "outputId": "f700b190-9120-46ae-8ed8-dea2c945f1d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-22 11:14:27--  https://raw.githubusercontent.com/CalvHobbes/ai-agents/main/agentic_rag_llamaindex_requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252 [text/plain]\n",
            "Saving to: ‘agentic_rag_llamaindex_requirements.txt’\n",
            "\n",
            "\r          agentic_r   0%[                    ]       0  --.-KB/s               \ragentic_rag_llamain 100%[===================>]     252  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-22 11:14:28 (3.74 MB/s) - ‘agentic_rag_llamaindex_requirements.txt’ saved [252/252]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the requirements\n",
        "!pip install -r agentic_rag_llamaindex_requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wn-0Ik2uE48",
        "outputId": "557b2bb6-e4d9-48ad-e72f-db4ce579ef40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv (from -r agentic_rag_llamaindex_requirements.txt (line 6))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting llama-index (from -r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_index-0.12.12-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-llms-openai (from -r agentic_rag_llamaindex_requirements.txt (line 9))\n",
            "  Downloading llama_index_llms_openai-0.3.13-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-embeddings-openai (from -r agentic_rag_llamaindex_requirements.txt (line 10))\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_index_agent_openai-0.4.2-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.12 (from llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_index_core-0.12.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_index_readers_file-0.4.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (3.9.1)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai->-r agentic_rag_llamaindex_requirements.txt (line 9)) (1.59.6)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (3.11.11)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (1.2.15)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2.10.5)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (1.17.0)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.8 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_cloud-0.1.9-py3-none-any.whl.metadata (911 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2024.11.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai->-r agentic_rag_llamaindex_requirements.txt (line 9)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai->-r agentic_rag_llamaindex_requirements.txt (line 9)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai->-r agentic_rag_llamaindex_requirements.txt (line 9)) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai->-r agentic_rag_llamaindex_requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->llama-index-llms-openai->-r agentic_rag_llamaindex_requirements.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8))\n",
            "  Downloading marshmallow-3.25.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (2024.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.12->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r agentic_rag_llamaindex_requirements.txt (line 8)) (1.17.0)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading llama_index-0.12.12-py3-none-any.whl (6.9 kB)\n",
            "Downloading llama_index_llms_openai-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_agent_openai-0.4.2-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.12-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.3-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.9-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.19-py3-none-any.whl (15 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.25.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, python-dotenv, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.9 llama-index-0.12.12 llama-index-agent-openai-0.4.2 llama-index-cli-0.4.0 llama-index-core-0.12.12 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.4 llama-index-llms-openai-0.3.13 llama-index-multi-modal-llms-openai-0.4.2 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.3 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.19 marshmallow-3.25.1 mypy-extensions-1.0.0 pypdf-5.1.0 python-dotenv-1.0.1 striprtf-0.0.26 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfAb9BqxdlZa",
        "outputId": "b7bc9a4c-dd33-46c2-aec8-f8dc6ff8e648"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-22 11:27:47--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‘metagpt.pdf’\n",
            "\n",
            "metagpt.pdf         100%[===================>]  16.13M  45.9MB/s    in 0.4s    \n",
            "\n",
            "2025-01-22 11:27:47 (45.9 MB/s) - ‘metagpt.pdf’ saved [16911937/16911937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n"
      ],
      "metadata": {
        "id": "ojVb0SKNeITj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "gIUmhpLSfV1i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAPI_KEY')"
      ],
      "metadata": {
        "id": "NQYtfBmvjR3_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "id": "1zgCJMVBykGt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "9caNdryxype-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "MjDkGpkN1xoB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "rduMKhHE20Z9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "EfQRgNQC3Ix2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwiepNUj3nQu",
        "outputId": "6ea06e33-daba-473b-f209-365e92812f44"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: This choice indicates that the document is useful for summarization questions related to MetaGPT..\n",
            "\u001b[0mThe document introduces MetaGPT, a meta-programming framework that enhances multi-agent systems based on Large Language Models (LLMs) through role specialization, workflow management, and efficient communication mechanisms. It incorporates an executable feedback mechanism to improve code generation quality during runtime and achieves state-of-the-art performance on various benchmarks. The document also provides insights into the development process of a software application called the \"Drawing App\" using MetaGPT, highlighting stages like requirement gathering, UI design, system architecture, implementation approach, testing, and task allocation. It discusses the use of Python libraries, creation of a user-friendly GUI, code generation, unit testing, and performance evaluation. Additionally, it touches upon challenges, ethical concerns, and the impact of MetaGPT on programming accessibility and transparency, while also mentioning potential future directions for the framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"How do agents share information with other agents?\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QSEGAqd4Mfm",
        "outputId": "9f420b99-48bd-43a3-95a4-a1433168c0fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it focuses on retrieving specific context from the MetaGPT paper, which may contain information on how agents share information with other agents..\n",
            "\u001b[0mAgents share information with other agents by utilizing a shared message pool where they can publish structured messages and subscribe to relevant messages based on their profiles. This shared message pool allows all agents to exchange messages directly, enabling them to access messages from other entities transparently. By storing information in this global message pool, agents can retrieve required information without the need to inquire about other agents and wait for their responses, thus enhancing communication efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3yut0C4rlW",
        "outputId": "85156612-23c8-48fe-f1ac-79935046e203"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: Ablation study results are specific context from the MetaGPT paper, making choice 2 the most relevant..\n",
            "\u001b[0mThe ablation study results show that MetaGPT effectively addresses challenges related to context utilization, code hallucinations, and information overload in the development of recommendation engines. By accurately unfolding natural language descriptions, maintaining information validity, and focusing on granular tasks like requirement analysis, MetaGPT mitigates issues such as ambiguity, incomplete code implementation, and irrelevant information. The use of a global message pool and subscription mechanism helps streamline communication and filter out irrelevant contexts, enhancing the efficiency and utility of the information provided.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"summarise ablation study\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMnFNYOw5aNa",
        "outputId": "e710c86d-1028-4c4f-9aa9-ca2de0fa1486"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: Ablation study is often used for summarization questions related to the performance of different components in a model like MetaGPT..\n",
            "\u001b[0mThe ablation study conducted in the MetaGPT project focused on systematically excluding different roles within the multi-agent system to evaluate their impact on the final results. The study demonstrated that the addition of specialized roles consistently improved the number of revisions needed and the executability of the generated code. Introducing more roles led to improved performance metrics, despite a slight increase in expenses, emphasizing the effectiveness of having diverse roles with specific expertise in the software development process within the MetaGPT framework.\n"
          ]
        }
      ]
    }
  ]
}