{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhqDOObcbnJgdzKHfyipG2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalvHobbes/ai-agents/blob/main/Agentic_RAG_with_LLamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9jGR-yoMUcRJ"
      },
      "outputs": [],
      "source": [
        "## REF - https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/2/router-query-engine"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file dynamically from GitHub\n",
        "!wget -O agentic_rag_llamaindex_requirements.txt https://raw.githubusercontent.com/CalvHobbes/ai-agents/main/agentic_rag_llamaindex_requirements.txt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRA48HlGUhx_",
        "outputId": "ca6fea5a-dd65-47a3-aa46-613c0b152567"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-23 08:46:59--  https://raw.githubusercontent.com/CalvHobbes/ai-agents/main/agentic_rag_llamaindex_requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 221 [text/plain]\n",
            "Saving to: ‘agentic_rag_llamaindex_requirements.txt’\n",
            "\n",
            "agentic_rag_llamain 100%[===================>]     221  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-23 08:47:00 (2.09 MB/s) - ‘agentic_rag_llamaindex_requirements.txt’ saved [221/221]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the requirements\n",
        "!pip install -r agentic_rag_llamaindex_requirements.txt"
      ],
      "metadata": {
        "id": "-Wn-0Ik2uE48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfAb9BqxdlZa",
        "outputId": "cbf4378b-919e-4ee7-fc38-09dd3ae20fcb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-23 08:47:10--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‘metagpt.pdf’\n",
            "\n",
            "metagpt.pdf         100%[===================>]  16.13M  42.2MB/s    in 0.4s    \n",
            "\n",
            "2025-01-23 08:47:11 (42.2 MB/s) - ‘metagpt.pdf’ saved [16911937/16911937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n"
      ],
      "metadata": {
        "id": "ojVb0SKNeITj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "gIUmhpLSfV1i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAPI_KEY')"
      ],
      "metadata": {
        "id": "NQYtfBmvjR3_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "id": "1zgCJMVBykGt"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Settings.llm"
      ],
      "metadata": {
        "id": "0_xxa6ZgRP8s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "9caNdryxype-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "MjDkGpkN1xoB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "rduMKhHE20Z9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "EfQRgNQC3Ix2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwiepNUj3nQu",
        "outputId": "bf3dd3cc-d6e7-4369-fbe4-9fa87b2dd6f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: This choice indicates that the document is useful for summarization questions related to MetaGPT..\n",
            "\u001b[0mThe document introduces MetaGPT, a meta-programming framework that enhances multi-agent systems based on Large Language Models (LLMs) through role specialization, workflow management, and efficient communication mechanisms. It includes agents like Product Managers, Architects, Engineers, and QA Engineers, each with specific roles. MetaGPT utilizes an executable feedback mechanism to improve code quality iteratively during runtime and outperforms existing approaches in code generation tasks. The document also provides insights into the development process of a software application, specifically a \"Drawing App,\" outlining requirements, UI design, implementation approach, required Python third-party packages, logic analysis, tasks breakdown, shared knowledge, and the involvement of various agents in the development cycle. It discusses the performance of MetaGPT in generating executable code, the impact of instruction levels on performance outcomes, challenges addressed by MetaGPT's designs, as well as limitations and ethics concerns related to the system and human users. Additionally, potential future enhancements like self-improvement mechanisms and multi-agent economies are considered to further advance MetaGPT's capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"How do agents share information with other agents?\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QSEGAqd4Mfm",
        "outputId": "441cfc16-b873-4cc0-d328-0640b4c31689"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it specifically mentions retrieving specific context, which is necessary for understanding how agents share information with other agents..\n",
            "\u001b[0mAgents share information with other agents by utilizing a shared message pool where they can publish structured messages and subscribe to relevant messages based on their profiles. This shared message pool allows all agents to exchange messages directly, access messages from other entities transparently, and retrieve required information without the need to inquire about other agents individually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3yut0C4rlW",
        "outputId": "d1c9ab1d-7180-407a-dd66-f28bd8707746"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: Ablation study results are specific context from the MetaGPT paper, making choice 2 the most relevant..\n",
            "\u001b[0mThe ablation study results show that MetaGPT effectively addresses challenges related to information overload and reduces hallucinations in software generation. By utilizing a global message pool and a subscription mechanism, MetaGPT efficiently manages information flow and filters out irrelevant contexts, enhancing the relevance and utility of the information. This design is crucial in optimizing communication and ensuring that the generated software programs are accurate and free from hallucination issues.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"summarise ablation study\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMnFNYOw5aNa",
        "outputId": "f6a915bc-1874-4977-fbd7-c2943daac00e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: Ablation study is often used for summarization questions related to the performance of different components in a model like MetaGPT..\n",
            "\u001b[0mThe ablation study examined the effects of different roles within the MetaGPT framework on software development outcomes. By systematically excluding roles, the study showed that role specialization improved software development processes by reducing revision costs and enhancing executability. Adding more roles led to better performance metrics. Additionally, the study evaluated MetaGPT's performance under various conditions, including initial input levels, detailed prompts, and different language models. Clear requirements and detailed prompts were found to enhance the quality of generated software projects, with specific language models like GPT-4 contributing to superior performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"who is gabriel cirulli\") # name onlt mentioned in an image in the metagpt pdf\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWRtm0sAJG7j",
        "outputId": "fd736ea0-a016-408b-eadd-f062d30c9783"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it focuses on retrieving specific context, which would likely include information about Gabriel Cirulli if he is mentioned in the MetaGPT paper..\n",
            "\u001b[0mGabriel Cirulli is not mentioned in the provided context information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function Calling ##"
      ],
      "metadata": {
        "id": "47LZl0zvCiZ8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def add(x: int, y: int) -> int:\n",
        "    \"\"\"Adds two integers together.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "def mystery(x: int, y: int) -> int:\n",
        "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
        "    return (x + y) * (x + y)\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(add)\n",
        "mystery_tool = FunctionTool.from_defaults(mystery)"
      ],
      "metadata": {
        "id": "IyPfzpP3Cj2S"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm.predict_and_call([add_tool, mystery_tool],\n",
        "    \"mystify 9 and 8\",\n",
        "    verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iGcWoArDFNt",
        "outputId": "2c0926e5-120c-45e0-9096-46f3b84c1c6a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: mystery with args: {\"x\": 9, \"y\": 8}\n",
            "=== Function Output ===\n",
            "289\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentChatResponse(response='289', sources=[ToolOutput(content='289', tool_name='mystery', raw_input={'args': (), 'kwargs': {'x': 9, 'y': 8}}, raw_output=289, is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nodes[33].get_content(metadata_mode=\"all\"))"
      ],
      "metadata": {
        "id": "Zl6Bko_AIZe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=4)\n"
      ],
      "metadata": {
        "id": "5KgFNhh3JnX-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What are some high-level results of MetaGPT?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4gjSispKK_N",
        "outputId": "20ebc383-b7d7-4505-c4b2-80fe2cf39e91"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MetaGPT outperforms all preceding approaches in both HumanEval and MBPP benchmarks. When MetaGPT collaborates with GPT-4, it significantly improves the Pass @k in the HumanEval benchmark compared to GPT-4. It achieves 85.9% and 87.7% pass rates on the MBPP and HumanEval with a single attempt. Additionally, MetaGPT achieves an average score of 3.9, surpassing ChatDev's score of 2.1, based on the Chat chain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.vector_stores import MetadataFilters\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=2,\n",
        "                                            filters=MetadataFilters.from_dicts(\n",
        "                                                [\n",
        "                                                    {\"key\": \"page_label\", \"value\": \"3\"}\n",
        "\n",
        "                                                  ])\n",
        ")\n"
      ],
      "metadata": {
        "id": "8NjKEvWAKmHN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"What are some high-level results of MetaGPT?\",\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaI7FKirL6zm",
        "outputId": "80d51a3b-38fa-4a47-8e51-551925e8c8ce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MetaGPT demonstrates state-of-the-art performance on HumanEval and MBPP, showcasing its effectiveness as a meta-programming framework for developing LLM-based multi-agent systems. Additionally, MetaGPT integrates human-like Standard Operating Procedures (SOPs) to enhance robustness and reduce unproductive collaboration among LLM-based agents. The framework also introduces an executive feedback mechanism that improves code generation quality significantly during runtime.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHC8JDDYL82Q",
        "outputId": "bf23aad3-fe49-40d5-ec0a-1cedf4439a76"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page_label': '3', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-23', 'last_modified_date': '2025-01-23'}\n",
            "{'page_label': '3', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-23', 'last_modified_date': '2025-01-23'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from llama_index.core.vector_stores import FilterCondition\n",
        "\n",
        "def total_page_count() -> int:\n",
        "  \"\"\"\n",
        "  Returns total number of pages in MetaGPT document, Only used to fetch the numnber of pages of the document\n",
        "  \"\"\"\n",
        "  return 29\n",
        "\n",
        "def vector_query(query:str, page_numbers: List[str]) -> str:\n",
        "  \"\"\"Perform a vector search over an index.\n",
        "  query (str): the string query to be embedded.\n",
        "  page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
        "  over all pages. Otherwise, filter by the set of specified pages.\n",
        "  \"\"\"\n",
        "  filters = [\n",
        "      {\"key\": \"page_label\", \"value\": page} for page in page_numbers\n",
        "  ]\n",
        "  query_engine = vector_index.as_query_engine(similarity_top_k=2,\n",
        "                                            filters=MetadataFilters.from_dicts(\n",
        "                                                filters,\n",
        "                                                condition=FilterCondition.OR\n",
        "                                                  )\n",
        "                                            )\n",
        "  response = query_engine.query(query)\n",
        "  return response\n",
        "\n",
        "vector_query_tool = FunctionTool.from_defaults( name=\"vector_query\",\n",
        "    fn=vector_query)\n",
        "page_count_tool = FunctionTool.from_defaults(name=\"page_count_tool\", fn=total_page_count)"
      ],
      "metadata": {
        "id": "MoL_7nlcMox9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_and_call([vector_query_tool], \"What are the high-level results of MetaGPT as described on the second page?\", verbose=True)"
      ],
      "metadata": {
        "id": "X0BQr2HTOYiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "# summary_index = SummaryIndex(nodes)\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "Rarow6PkTz8m"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What are the MetaGPT comparisons with ChatDev described on second page?\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCLRGtMWWmtL",
        "outputId": "a4b6c8b6-be59-435e-f948-6c41e9985dd5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_query with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"2\"]}\n",
            "=== Function Output ===\n",
            "MetaGPT outperforms ChatDev in handling higher levels of software complexity and offering extensive functionality. In experimental evaluations, MetaGPT achieves a 100% task completion rate, showcasing the robustness and efficiency of its design in terms of time and token costs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zBUlZd2WsAw",
        "outputId": "1df8f75e-5d99-4bf0-b60f-e4771988b654"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-23', 'last_modified_date': '2025-01-23'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"what is the summary of the paper?\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3wOrmOIW3rz",
        "outputId": "b0a56ef8-2a9c-4fa7-e154-d2628b2dba13"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"summary of the paper\"}\n",
            "=== Function Output ===\n",
            "The paper introduces MetaGPT, a meta-programming framework that utilizes Standardized Operating Procedures (SOPs) to enhance multi-agent systems based on Large Language Models (LLMs). It incorporates role specialization, workflow management, and efficient communication mechanisms to improve problem-solving capabilities. MetaGPT employs an executable feedback mechanism to enhance code generation quality during runtime, outperforming previous approaches in various benchmarks. The paper discusses the development process of a software application called the \"Drawing App\" using MetaGPT, outlining tasks like designing a user-friendly GUI, implementing color selection functionality, displaying RGB values in real-time, and testing for accuracy and performance. It also addresses the performance of MetaGPT in generating executable code, the impact of different instruction levels on task outcomes, challenges like information overload and code hallucinations, as well as limitations and ethical concerns related to the system's use. Additionally, potential future directions such as self-improvement mechanisms and multi-agent economies are discussed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "BncKaCfgewAd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    [vector_tool, summary_tool, page_count_tool],\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ],
      "metadata": {
        "id": "ssTA02-Fb6uO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the agent roles in MetaGPT, \"\n",
        "    \"and then how they communicate with each other.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "125KdClkdUdt",
        "outputId": "2780353a-62d4-44a2-f245-5d1d2d45fc79"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"Agent roles in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The agent roles in MetaGPT include the Product Manager, Architect, Project Manager, Engineer, and QA Engineer. The Product Manager is responsible for generating the Product Requirement Document (PRD) and competitive analysis. The Architect designs the technical specifications, system architecture, and interface definitions. The Project Manager breaks down the project into tasks and assigns them to Engineers. The Engineer develops the code based on the specifications provided. The QA Engineer generates unit tests and reviews the code for bugs to ensure high-quality software. Each role plays a crucial part in the software development process within the MetaGPT framework.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"Communication between agent roles in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The communication between agent roles in MetaGPT is structured and efficient, with each agent performing specific tasks based on the information provided by the previous agent. The Product Manager generates a Product Requirement Document (PRD) outlining goals, user stories, competitive analysis, and requirements, which is then passed to the Architect for system architecture and interface design. The Architect's documentation is then handed over to the Project Manager for task breakdown and assignment to the Engineers. The Engineers develop the code based on the specifications provided, and the QA Engineer generates unit test code to ensure software quality. This sequential flow of information and tasks ensures a collaborative and effective process within MetaGPT.\n",
            "=== LLM Response ===\n",
            "The agent roles in MetaGPT include the Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities in the software development process. The Product Manager generates the Product Requirement Document (PRD) and competitive analysis. The Architect designs the technical specifications and system architecture. The Project Manager breaks down tasks and assigns them to Engineers. The Engineers develop the code based on the specifications, and the QA Engineer ensures software quality by generating unit tests and reviewing the code for bugs.\n",
            "\n",
            "Communication between agent roles in MetaGPT is structured and efficient. The Product Manager provides the PRD to the Architect, who then designs the system architecture. The Architect's documentation is passed to the Project Manager for task assignment to Engineers. The Engineers develop the code based on the specifications, and the QA Engineer ensures software quality through unit tests. This sequential flow of information and tasks ensures a collaborative and effective process within MetaGPT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.chat(\n",
        "    \"Tell me about the evaluation datasets used.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AX7Ksyddwx2",
        "outputId": "38ead16c-9bf1-425b-e86b-22789d14d10a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the evaluation datasets used.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"Evaluation datasets used in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The evaluation datasets used in MetaGPT include HumanEval, MBPP, and SoftwareDev. HumanEval consisted of 164 handwritten programming tasks, MBPP included 427 Python tasks covering core concepts and standard library features, and SoftwareDev comprised 70 representative examples of software development tasks with diverse scopes such as mini-games, image processing algorithms, and data visualization.\n",
            "=== LLM Response ===\n",
            "The evaluation datasets used in MetaGPT include HumanEval, MBPP, and SoftwareDev. \n",
            "\n",
            "1. HumanEval: This dataset consisted of 164 handwritten programming tasks.\n",
            "2. MBPP: This dataset included 427 Python tasks covering core concepts and standard library features.\n",
            "3. SoftwareDev: This dataset comprised 70 representative examples of software development tasks with diverse scopes, such as mini-games, image processing algorithms, and data visualization.\n",
            "\n",
            "These datasets were used to evaluate the performance and capabilities of MetaGPT in various programming tasks and scenarios.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.chat(\"Tell me the results over one of the above datasets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu0c6e-oiFmU",
        "outputId": "cfdeb6e4-038a-4a53-f169-2c0383a2723b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me the results over one of the above datasets.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"Results over HumanEval dataset in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "MetaGPT achieved state-of-the-art performance on the HumanEval dataset, surpassing previous approaches in solving programming tasks. GPT-4 demonstrated higher sensitivity to prompts, code parsing, and post-processing compared to GPT-3.5-Turbo. Across various settings, GPT-4 consistently outperformed GPT-3.5-Turbo in terms of completion accuracy and reliability.\n",
            "=== LLM Response ===\n",
            "MetaGPT achieved state-of-the-art performance on the HumanEval dataset, surpassing previous approaches in solving programming tasks. GPT-4 demonstrated higher sensitivity to prompts, code parsing, and post-processing compared to GPT-3.5-Turbo. Across various settings, GPT-4 consistently outperformed GPT-3.5-Turbo in terms of completion accuracy and reliability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task = agent.create_task(\n",
        "    \"Tell me about the agent roles in MetaGPT, \"\n",
        "    \"and then how they communicate with each other.\"\n",
        ")"
      ],
      "metadata": {
        "id": "oCvCrEOxhU0p"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_output = agent.run_step(task.task_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUkPVjtSd0ja",
        "outputId": "6ec0d932-d6d5-4157-c79a-40f461d61628"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"Agent roles in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The roles in MetaGPT include the Product Manager responsible for creating the Product Requirement Document and analyzing user stories, the Architect who designs system architecture and technical specifications, the Project Manager who breaks down tasks and assigns them to Engineers, the Engineers who develop the code based on specifications, and the QA Engineer who generates unit tests and ensures software quality. Each agent has a specific role in the software development process within the MetaGPT framework.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"Communication between agent roles in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "Communication between agent roles in MetaGPT is structured and efficient, with each agent performing specific tasks based on the information provided by the previous agent. The Product Manager generates the initial document outlining goals and user stories, which is then passed to the Architect for system architecture and interface design. The Project Manager breaks down the project into tasks based on these specifications, which are then implemented by the Engineer. The QA Engineer ensures software quality by generating unit tests based on the output from the Engineer. This sequential flow of information and tasks ensures clear and effective collaboration among the agents in MetaGPT.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "for key in dict(step_output).keys():\n",
        "    print(key)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGtrbskhks6n",
        "outputId": "e1dad82a-6603-48d2-a441-300d6cf0e74b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output\n",
            "task_step\n",
            "next_steps\n",
            "is_last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completed_steps = agent.get_completed_steps(task.task_id)\n",
        "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
        "print(completed_steps[0].output.sources[0].raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moeijd3Hk_HU",
        "outputId": "82bb5176-72c4-49b2-bf1d-ec6ac8677971"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num completed for task 2c0a9e4c-2152-4315-a6c8-ee49feef75f0: 1\n",
            "The roles in MetaGPT include the Product Manager responsible for creating the Product Requirement Document and analyzing user stories, the Architect who designs system architecture and technical specifications, the Project Manager who breaks down tasks and assigns them to Engineers, the Engineers who develop the code based on specifications, and the QA Engineer who generates unit tests and ensures software quality. Each agent has a specific role in the software development process within the MetaGPT framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
        "print(f\"Num upcoming steps for task {task.task_id}: {len(upcoming_steps)}\")\n",
        "upcoming_steps[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnY2gBhflJOI",
        "outputId": "829c9aed-a92b-458b-853e-f39d816fb9b3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num upcoming steps for task 2c0a9e4c-2152-4315-a6c8-ee49feef75f0: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskStep(task_id='2c0a9e4c-2152-4315-a6c8-ee49feef75f0', step_id='101cc444-fa57-4dd5-ae3a-c710b5701e12', input=None, step_state={}, next_steps={}, prev_steps={}, is_ready=True)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_output = agent.run_step(\n",
        "    task.task_id, input=\"What about how agents share information?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4G4ZEkbl3sX",
        "outputId": "34ad682d-b635-41cf-f863-687586b550b8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What about how agents share information?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"How agents share information in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "Agents in MetaGPT share information through a structured communication protocol involving publishing structured messages in a shared message pool and subscribing to relevant messages based on their profiles. This method allows for efficient exchange of information without direct one-to-one communication, enhancing communication efficiency. The subscription mechanism enables agents to select and follow information based on their role profiles, ensuring they receive only task-relevant information and avoid distractions from irrelevant details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_output = agent.run_step(task.task_id)\n",
        "print(step_output.is_last)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxJIm7pnAiL",
        "outputId": "2cbfb21c-3440-4056-a656-94bb9dc108c8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LLM Response ===\n",
            "Agents in MetaGPT share information through a structured communication protocol. They publish structured messages in a shared message pool and subscribe to relevant messages based on their profiles. This method allows for efficient exchange of information without direct one-to-one communication, enhancing communication efficiency. The subscription mechanism enables agents to select and follow information based on their role profiles, ensuring they receive only task-relevant information and avoid distractions from irrelevant details.\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.finalize_response(task.task_id)\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4Sd2IKhotqx",
        "outputId": "a4b6b231-c155-40f1-9762-cf2a973b766f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agents in MetaGPT share information through a structured communication protocol. They publish structured messages in a shared message pool and subscribe to relevant messages based on their profiles. This method allows for efficient exchange of information without direct one-to-one communication, enhancing communication efficiency. The subscription mechanism enables agents to select and follow information based on their role profiles, ensuring they receive only task-relevant information and avoid distractions from irrelevant details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Document RAG##"
      ],
      "metadata": {
        "id": "eijEoIFsq9en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "]"
      ],
      "metadata": {
        "id": "6Zx4XxV-rAVQ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# for url, filename in zip(urls, papers):\n",
        "#     response = requests.get(url)\n",
        "#     response.raise_for_status()  # Raise an exception for bad responses\n",
        "\n",
        "#     # Save the PDF to the 'pdfs' directory\n",
        "#     filepath = os.path.join(\"\", filename)\n",
        "#     with open(filepath, \"wb\") as f:\n",
        "#         f.write(response.content)\n",
        "\n",
        "#     print(f\"Downloaded {filename} from {url}\")\n",
        "\n",
        "# print(\"All PDFs downloaded successfully!\")\n",
        "for url, paper in zip(urls, papers):\n",
        "     !wget \"{url}\" -O \"{paper}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgnJGnJJrOqM",
        "outputId": "4ff3725f-8068-443e-e82c-9f5ff9aa0060"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded metagpt.pdf from https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Downloaded longlora.pdf from https://openreview.net/pdf?id=6PmJoRfdaK\n",
            "Downloaded selfrag.pdf from https://openreview.net/pdf?id=hSyW5go0v8\n",
            "All PDFs downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional,List\n",
        "from llama_index.core.vector_stores import FilterCondition\n",
        "def get_doc_tools(\n",
        "    file_path: str,\n",
        "    name: str,\n",
        ") -> str:\n",
        "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
        "\n",
        "    # load documents\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "    vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "    def vector_query(\n",
        "        query: str,\n",
        "        page_numbers: Optional[List[str]] = None\n",
        "    ) -> str:\n",
        "        \"\"\"Use to answer questions over a given paper.\n",
        "\n",
        "        Useful if you have specific questions over the paper.\n",
        "        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
        "\n",
        "        Args:\n",
        "            query (str): the string query to be embedded.\n",
        "            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\n",
        "                if we want to perform a vector search\n",
        "                over all pages. Otherwise, filter by the set of specified pages.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        page_numbers = page_numbers or []\n",
        "        metadata_dicts = [\n",
        "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "        ]\n",
        "\n",
        "        query_engine = vector_index.as_query_engine(\n",
        "            similarity_top_k=2,\n",
        "            filters=MetadataFilters.from_dicts(\n",
        "                metadata_dicts,\n",
        "                condition=FilterCondition.OR\n",
        "            )\n",
        "        )\n",
        "        response = query_engine.query(query)\n",
        "        return response\n",
        "\n",
        "\n",
        "    vector_query_tool = FunctionTool.from_defaults(\n",
        "        name=f\"vector_tool_{name}\",\n",
        "        fn=vector_query\n",
        "    )\n",
        "\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        name=f\"summary_tool_{name}\",\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            f\"Useful for summarization questions related to {name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return vector_query_tool, summary_tool"
      ],
      "metadata": {
        "id": "OiIFKVMmroyv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLuVKHF1spsC",
        "outputId": "c1c7773a-aee0-439e-9397-d85d8d54a02d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: selfrag.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ],
      "metadata": {
        "id": "roFcgP7tW4Vv"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_doc_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    initial_tools,\n",
        "    llm=model,\n",
        "    verbose=True\n",
        ")\n",
        "multi_doc_agent = AgentRunner(multi_doc_worker)"
      ],
      "metadata": {
        "id": "MmDgZqooYQcl"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = multi_doc_agent.query(\n",
        "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "    \"and then tell me about the evaluation results\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP83TMAiYnB4",
        "outputId": "b1864a78-d7b9-4d8f-9cfc-400a364f9573"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "PG19 test split\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
            "=== Function Output ===\n",
            "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity scores. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
            "=== LLM Response ===\n",
            "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
            "\n",
            "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity scores. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = multi_doc_agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
        "# print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMIKm7CWZTss",
        "outputId": "8461bc90-ec6d-4245-c427-4f1246012341"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
            "=== Function Output ===\n",
            "Self-RAG is a framework that enhances the quality and factuality of large language models by utilizing retrieval on demand and self-reflection. It involves training a single LM to retrieve, generate, and critique text passages using reflection tokens. This framework allows for customization of LM behaviors at test time, leading to improved performance, factuality, and citation accuracy compared to other models. Additionally, Self-RAG evaluates text generation outputs based on fine-grained aspects like factual relevance, supportiveness, and overall usefulness, aiming to ensure that generated text is factually accurate, well-supported by evidence, and provides informative answers to queries.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
            "=== Function Output ===\n",
            "LongLoRA is a framework that extends the context length of large language models with minimal accuracy compromise by introducing Shifted Sparse Attention (S2-Attn) during training. It retains the original attention architecture during inference, making it compatible with existing infrastructure and optimization techniques. LongLoRA includes the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) components, focusing on modeling relations between facial action units and providing Local Tampering Supervision for forgery detection. The framework achieves state-of-the-art performance on evaluations across datasets and manipulation methods, offering visualizations of tampered regions to aid in understanding manipulation processes.\n",
            "=== LLM Response ===\n",
            "Self-RAG is a framework that enhances the quality and factuality of large language models by utilizing retrieval on demand and self-reflection. It involves training a single LM to retrieve, generate, and critique text passages using reflection tokens. This framework allows for customization of LM behaviors at test time, leading to improved performance, factuality, and citation accuracy compared to other models. Additionally, Self-RAG evaluates text generation outputs based on fine-grained aspects like factual relevance, supportiveness, and overall usefulness, aiming to ensure that generated text is factually accurate, well-supported by evidence, and provides informative answers to queries.\n",
            "\n",
            "LongLoRA is a framework that extends the context length of large language models with minimal accuracy compromise by introducing Shifted Sparse Attention (S2-Attn) during training. It retains the original attention architecture during inference, making it compatible with existing infrastructure and optimization techniques. LongLoRA includes the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) components, focusing on modeling relations between facial action units and providing Local Tampering Supervision for forgery detection. The framework achieves state-of-the-art performance on evaluations across datasets and manipulation methods, offering visualizations of tampered regions to aid in understanding manipulation processes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Doc Solution\n",
        "Can't use tools directly for large number of docs due to context length restrictions on model. Instead, use RAG over tools to fetch the right tools for the llm using ObjectIndex in LlamaIndex."
      ],
      "metadata": {
        "id": "AUr-CgiOdnFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    \"zipformer.pdf\",\n",
        "    \"values.pdf\",\n",
        "    \"finetune_fair_diffusion.pdf\",\n",
        "    \"knowledge_card.pdf\"\n",
        "]"
      ],
      "metadata": {
        "id": "thOoTvpsd06L"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for url, paper in zip(urls, papers):\n",
        "     !wget \"{url}\" -O \"{paper}\""
      ],
      "metadata": {
        "id": "gMbnfNppd2QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2mCX3X8eA8r",
        "outputId": "4fd49f57-887e-47db-88b6-53e67f2956fb"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: loftq.pdf\n",
            "Getting tools for paper: swebench.pdf\n",
            "Getting tools for paper: selfrag.pdf\n",
            "Getting tools for paper: zipformer.pdf\n",
            "Getting tools for paper: values.pdf\n",
            "Getting tools for paper: finetune_fair_diffusion.pdf\n",
            "Getting tools for paper: knowledge_card.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in paper_to_tools_dict.values():\n",
        "   [vector_tool, summary_tool] = item[0], item[1]\n",
        "   print(vector_tool.metadata)\n",
        "   print(summary_tool.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awCfvmdDnk5M",
        "outputId": "4cb62408-3904-49e4-9a7f-044e1cbd895b"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ToolMetadata(description='vector_tool_metagpt(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over a given paper.\\n    \\n        Useful if you have specific questions over the paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n    \\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n        \\n        ', name='vector_tool_metagpt', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_metagpt'>, return_direct=False)\n",
            "ToolMetadata(description='Useful for summarization questions related to metagpt', name='summary_tool_metagpt', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n",
            "ToolMetadata(description='vector_tool_longlora(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over a given paper.\\n    \\n        Useful if you have specific questions over the paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n    \\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n        \\n        ', name='vector_tool_longlora', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_longlora'>, return_direct=False)\n",
            "ToolMetadata(description='Useful for summarization questions related to longlora', name='summary_tool_longlora', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n",
            "ToolMetadata(description='vector_tool_loftq(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over a given paper.\\n    \\n        Useful if you have specific questions over the paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n    \\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n        \\n        ', name='vector_tool_loftq', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_loftq'>, return_direct=False)\n",
            "ToolMetadata(description='Useful for summarization questions related to loftq', name='summary_tool_loftq', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n",
            "ToolMetadata(description='vector_tool_swebench(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over a given paper.\\n    \\n        Useful if you have specific questions over the paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n    \\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n        \\n        ', name='vector_tool_swebench', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_swebench'>, return_direct=False)\n",
            "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n",
            "ToolMetadata(description='vector_tool_selfrag(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over a given paper.\\n    \\n        Useful if you have specific questions over the paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n    \\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n        \\n        ', name='vector_tool_selfrag', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_selfrag'>, return_direct=False)\n",
            "ToolMetadata(description='Useful for summarization questions related to selfrag', name='summary_tool_selfrag', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n",
            "ToolMetadata(description='vector_tool_zipformer(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over a given paper.\\n    \\n        Useful if you have specific questions over the paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n    \\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n        \\n        ', name='vector_tool_zipformer', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_zipformer'>, return_direct=False)\n",
            "ToolMetadata(description='Useful for summarization questions related to zipformer', name='summary_tool_zipformer', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n",
            "ToolMetadata(description='vector_tool_values(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over a given paper.\\n    \\n        Useful if you have specific questions over the paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n    \\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n        \\n        ', name='vector_tool_values', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_values'>, return_direct=False)\n",
            "ToolMetadata(description='Useful for summarization questions related to values', name='summary_tool_values', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n",
            "ToolMetadata(description='vector_tool_finetune_fair_diffusion(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over a given paper.\\n    \\n        Useful if you have specific questions over the paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n    \\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n        \\n        ', name='vector_tool_finetune_fair_diffusion', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_finetune_fair_diffusion'>, return_direct=False)\n",
            "ToolMetadata(description='Useful for summarization questions related to finetune_fair_diffusion', name='summary_tool_finetune_fair_diffusion', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n",
            "ToolMetadata(description='vector_tool_knowledge_card(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over a given paper.\\n    \\n        Useful if you have specific questions over the paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n    \\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n        \\n        ', name='vector_tool_knowledge_card', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_knowledge_card'>, return_direct=False)\n",
            "ToolMetadata(description='Useful for summarization questions related to knowledge_card', name='summary_tool_knowledge_card', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ],
      "metadata": {
        "id": "eqRcXRXdosbw"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in all_tools:\n",
        "  print (t.metadata)"
      ],
      "metadata": {
        "id": "54sLas2Sh109"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "tools_index = ObjectIndex.from_objects(all_tools,\n",
        "                                       index_cls=VectorStoreIndex)\n",
        "\n",
        "obj_retriever = tools_index.as_retriever(similarity_top_k=3)\n"
      ],
      "metadata": {
        "id": "Ej03pEshfYOP"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TO DEBUG Index contents\n",
        "# # Retrieve all node IDs from the docstore\n",
        "# doc_store = tools_index.index.docstore\n",
        "# node_ids = list(doc_store.docs.keys())\n",
        "\n",
        "# # Fetch the nodes using the retrieved node IDs\n",
        "# nodes = doc_store.get_nodes(node_ids)\n",
        "\n",
        "# # Print the text content of each node\n",
        "# for node in nodes:\n",
        "#     print(node.get_text())\n",
        "\n"
      ],
      "metadata": {
        "id": "aBKffGnKmjE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# specific_tools = obj_retriever.retrieve(\n",
        "#     \"Tell me about longlora\"\n",
        "# )\n"
      ],
      "metadata": {
        "id": "cHTX8ZYngHMW"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [print(t.metadata) for t in specific_tools]"
      ],
      "metadata": {
        "id": "7pTE8nJcgbDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_retriever,\n",
        "    llm=llm,\n",
        "    system_prompt=\"\"\" \\\n",
        "You are an agent designed to answer queries over a set of given papers.\n",
        "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\n",
        "\"\"\",\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ],
      "metadata": {
        "id": "A-13tg5Ug1vC"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\n",
        "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
        "    \"Analyze the approach in each paper first. \"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-OQPf7Sieja",
        "outputId": "63cf7ebd-5487-494f-8400-22cdc5444768"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"Analyzing the approach in the LongLoRA paper.\"}\n",
            "=== Function Output ===\n",
            "The approach in the LongLoRA paper introduces an efficient fine-tuning method that extends the context length of large language models by combining the LoRA method with shifted sparse attention during training. This approach enables models to be fine-tuned with minimal accuracy compromise while emphasizing the importance of trainable normalization and embedding layers for successful long context adaptation. Additionally, the paper introduces the Action Units Relation Learning framework, which includes the ART encoder for capturing intra-face relations for forgery detection and the TAP process for generating challenging pseudo-samples to enhance model generalization. The paper achieves state-of-the-art performance in cross-dataset and cross-manipulation evaluations, with qualitative visualizations aiding in understanding how different regions are modified during the manipulation process. The main contributions of the approach lie in the modeling of intra-face relations by the ART encoder and the generation of challenging pseudo-samples by the TAP process, collectively improving the generalization of the deepfake detection model.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_loftq with args: {\"input\": \"Analyzing the approach in the LoftQ paper.\"}\n",
            "=== Function Output ===\n",
            "The LoftQ paper presents a novel quantization framework that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. This approach aims to provide a better initialization for subsequent LoRA fine-tuning, resulting in improved performance on downstream tasks. LoftQ demonstrates effectiveness and robustness, particularly in low-bit quantization scenarios, surpassing existing methods like QLoRA. The alternating optimization strategy employed in LoftQ helps minimize the gap between quantized weights and pre-trained weights, leading to enhanced fine-tuning performance. Furthermore, LoftQ showcases promising results across various tasks such as natural language understanding, question answering, summarization, and natural language generation. The focus of the LoftQ approach is on quantization techniques for model compression, with a specific target on the DeBERTaV3-base model. By incorporating low-rank adapters, LoftQ can extend its application to convolutional layers, enabling efficient compression of neural network models. LoftQ also outperforms state-of-the-art pruning methods in terms of model performance and memory savings, making it a compelling approach for model compression in natural language processing tasks.\n",
            "=== LLM Response ===\n",
            "The LongLoRA paper introduces an efficient fine-tuning method that extends the context length of large language models by combining the LoRA method with shifted sparse attention during training. It emphasizes the importance of trainable normalization and embedding layers for successful long context adaptation. The paper also introduces the Action Units Relation Learning framework, including the ART encoder for capturing intra-face relations for forgery detection and the TAP process for generating challenging pseudo-samples to enhance model generalization. LongLoRA achieves state-of-the-art performance in cross-dataset and cross-manipulation evaluations, with a focus on modeling intra-face relations and generating challenging pseudo-samples to improve model generalization.\n",
            "\n",
            "On the other hand, the LoftQ paper presents a novel quantization framework that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. This framework aims to provide a better initialization for subsequent LoRA fine-tuning, resulting in improved performance on downstream tasks. LoftQ demonstrates effectiveness and robustness, especially in low-bit quantization scenarios, surpassing existing methods like QLoRA. The alternating optimization strategy in LoftQ minimizes the gap between quantized weights and pre-trained weights, leading to enhanced fine-tuning performance. LoftQ showcases promising results across various tasks such as natural language understanding, question answering, summarization, and natural language generation, with a focus on quantization techniques for model compression and extending its application to convolutional layers for efficient model compression.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used \"\n",
        "    \"in MetaGPT and compare it against SWE-Bench\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA1uEn17g476",
        "outputId": "18c5ffcb-ea42-4072-d2ef-536933924ebf"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in MetaGPT includes the HumanEval benchmark, the MBPP benchmark, and the SoftwareDev dataset. The HumanEval benchmark consists of 164 handwritten programming tasks, the MBPP benchmark comprises 427 Python tasks, and the SoftwareDev dataset contains 70 representative examples of software development tasks with diverse scopes.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in SWE-Bench is constructed by scraping pull requests from the top 100 most downloaded PyPI libraries. Task instances are created from merged pull requests that resolve issues and introduce new tests. The dataset includes codebase snapshots, descriptions of the issues to be resolved, and the associated pull request's code changes. It is continuously updated with new task instances from popular repositories to provide a diverse and challenging set of problems for language models to solve.\n",
            "=== LLM Response ===\n",
            "The evaluation dataset used in MetaGPT includes the HumanEval benchmark, the MBPP benchmark, and the SoftwareDev dataset. The HumanEval benchmark consists of 164 handwritten programming tasks, the MBPP benchmark comprises 427 Python tasks, and the SoftwareDev dataset contains 70 representative examples of software development tasks with diverse scopes.\n",
            "\n",
            "On the other hand, the evaluation dataset used in SWE-Bench is constructed by scraping pull requests from the top 100 most downloaded PyPI libraries. Task instances are created from merged pull requests that resolve issues and introduce new tests. The dataset includes codebase snapshots, descriptions of the issues to be resolved, and the associated pull request's code changes. It is continuously updated with new task instances from popular repositories to provide a diverse and challenging set of problems for language models to solve.\n",
            "The evaluation dataset used in MetaGPT includes the HumanEval benchmark, the MBPP benchmark, and the SoftwareDev dataset. The HumanEval benchmark consists of 164 handwritten programming tasks, the MBPP benchmark comprises 427 Python tasks, and the SoftwareDev dataset contains 70 representative examples of software development tasks with diverse scopes.\n",
            "\n",
            "On the other hand, the evaluation dataset used in SWE-Bench is constructed by scraping pull requests from the top 100 most downloaded PyPI libraries. Task instances are created from merged pull requests that resolve issues and introduce new tests. The dataset includes codebase snapshots, descriptions of the issues to be resolved, and the associated pull request's code changes. It is continuously updated with new task instances from popular repositories to provide a diverse and challenging set of problems for language models to solve.\n"
          ]
        }
      ]
    }
  ]
}